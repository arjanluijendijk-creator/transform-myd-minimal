#!/usr/bin/env python3
"""
YAML generation and file handling for transform-myd-minimal.

Contains all YAML generation logic and file operations including:
- Column mapping YAML generation
- Fields YAML generation  
- Value rules YAML generation
- Object list YAML generation
- Migration structure generation
"""

import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
import yaml

from .logging_config import get_logger

# Initialize logger for this module
logger = get_logger(__name__)

from .fuzzy import FuzzyConfig


def read_excel_fields(excel_path):
    """Read the Excel file and extract source and target fields"""
    try:
        df = pd.read_excel(excel_path)
    except FileNotFoundError:
        raise FileNotFoundError(f"Excel file not found: {excel_path}")
    except Exception as e:
        raise Exception(f"Error reading Excel file: {e}")
    
    # Split into source and target fields
    source_fields = df[df['field'] == 'Source'].copy()
    target_fields = df[df['field'] == 'Target'].copy()
    
    return source_fields, target_fields


def scan_data_structure(base_path, output_dir="output"):
    """Scan output_dir/{object}/{variant} structure to find all objects and tables."""
    objects = {}
    config_path = base_path / output_dir
    
    if not config_path.exists():
        logger.warning(f"{config_path} does not exist")
        return {}
    
    # Scan for objects and variants
    for object_dir in config_path.iterdir():
        if object_dir.is_dir() and not object_dir.name.startswith('.'):
            object_name = object_dir.name
            tables = []
            
            for variant_dir in object_dir.iterdir():
                if variant_dir.is_dir() and not variant_dir.name.startswith('.'):
                    tables.append(variant_dir.name)
            
            if tables:
                objects[object_name] = sorted(tables)
    
    return objects


def generate_object_list_yaml(base_path, output_dir="output"):
    """Generate object_list.yaml with overview of all objects and tables."""
    objects_structure = scan_data_structure(base_path, output_dir)
    
    # Format for YAML output
    yaml_data = {
        'Objects': []
    }
    
    for object_name, tables in objects_structure.items():
        yaml_data['Objects'].append({
            'object': object_name,
            'tables': tables
        })
    
    # Write to file
    output_path = base_path / output_dir / "object_list.yaml"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(f"# Generated by transform-myd-minimal @ {datetime.now().strftime('%Y%m%d %H%M')}\n")
        f.write("# Overview of all objects and their tables\n\n")
        yaml.dump(yaml_data, f, default_flow_style=False, allow_unicode=True)
    
    logger.info(f"Generated: {output_path}")
    return output_path


def determine_field_type(field_data):
    """Determine field type based on field properties."""
    # Default type
    field_type = "string"
    
    # You can extend this logic based on field naming patterns or descriptions
    field_name = field_data.get('field_name', '').lower()
    field_desc = field_data.get('field_description', '').lower()
    
    if any(keyword in field_name for keyword in ['date', 'dat', 'time']):
        field_type = "date"
    elif any(keyword in field_name for keyword in ['amount', 'amt', 'value', 'val']):
        field_type = "decimal"
    elif any(keyword in field_name for keyword in ['count', 'number', 'num', 'id']):
        field_type = "integer"
    elif any(keyword in field_desc for keyword in ['boolean', 'flag', 'indicator']):
        field_type = "boolean"
    
    return field_type


def is_operational_field(field_name, field_description):
    """Determine if a field is operational based on name/description patterns."""
    operational_patterns = [
        'flag', 'status', 'indicator', 'control', 'system', 'process',
        'update', 'created', 'modified', 'version', 'lock'
    ]
    
    field_text = f"{field_name} {field_description}".lower()
    return any(pattern in field_text for pattern in operational_patterns)


def is_derived_field(field_name, field_description):
    """Determine if a field is derived based on name/description patterns."""
    derived_patterns = [
        'calculated', 'computed', 'derived', 'total', 'sum', 'average',
        'balance', 'amount', 'percentage', 'ratio'
    ]
    
    field_text = f"{field_name} {field_description}".lower()
    return any(pattern in field_text for pattern in derived_patterns)


def generate_fields_yaml(base_path, object_name, variant, df, output_dir="output", input_dir="data/02_fields"):
    """Generate fields.yaml for a specific table (object/variant combination)."""
    if df is None:
        return None
    
    # Filter for target fields (these are the output fields)
    target_fields = df[df['field'] == 'Target'].copy()
    
    # Generate fields YAML structure
    fields_data = {
        'table': f"{object_name}_{variant}",
        'fields': []
    }
    
    for _, row in target_fields.iterrows():
        field_info = {
            'name': row.get('field_name', ''),
            'description': row.get('field_description', ''),
            'type': determine_field_type(row),
            'required': bool(row.get('field_is_mandatory', False)),
            'key': bool(row.get('field_is_key', False))
        }
        fields_data['fields'].append(field_info)
    
    # Write to file
    excel_filename = f"fields_{object_name}_{variant}.xlsx"
    output_path = base_path / output_dir / object_name / variant / "fields.yaml"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(f"# Generated by transform-myd-minimal @ {datetime.now().strftime('%Y%m%d %H%M')}\n")
        f.write(f"# Field definitions for {object_name}_{variant}\n")
        f.write(f"# Source: {input_dir}/{excel_filename}\n\n")
        yaml.dump(fields_data, f, default_flow_style=False, allow_unicode=True)
    
    logger.info(f"Generated: {output_path}")
    return output_path


def generate_value_rules_yaml(base_path, object_name, variant, df, output_dir="output", input_dir="data/02_fields"):
    """Generate value_rules.yaml for a specific table."""
    if df is None:
        return None
    
    # Filter for target fields
    target_fields = df[df['field'] == 'Target'].copy()
    
    # Generate rules YAML structure
    rules_data = {
        'table': f"{object_name}_{variant}",
        'value_rules': []
    }
    
    for _, row in target_fields.iterrows():
        field_name = row.get('field_name', '')
        field_description = row.get('field_description', '')
        is_mandatory = bool(row.get('field_is_mandatory', False))
        
        rule_info = {
            'field': field_name,
            'description': field_description
        }
        
        # Determine rule type based on field properties
        if is_mandatory:
            rule_info['rule'] = 'required'
            rule_info['reason'] = 'Mandatory field as per data specification'
        elif is_operational_field(field_name, field_description):
            rule_info['rule'] = 'constant'
            rule_info['value'] = ' '  # Empty/blank default
            rule_info['reason'] = 'Operational field; no semantic source in source data'
        elif is_derived_field(field_name, field_description):
            rule_info['rule'] = 'derive'
            rule_info['reason'] = 'Derived field; requires business logic implementation'
        else:
            rule_info['rule'] = 'map'
            rule_info['reason'] = 'Direct mapping from source field'
        
        rules_data['value_rules'].append(rule_info)
    
    # Write to file
    excel_filename = f"fields_{object_name}_{variant}.xlsx"
    output_path = base_path / output_dir / object_name / variant / "value_rules.yaml"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(f"# Generated by transform-myd-minimal @ {datetime.now().strftime('%Y%m%d %H%M')}\n")
        f.write(f"# Value rules for {object_name}_{variant}\n")
        f.write(f"# Rules: required, constant, derive, map\n")
        f.write(f"# Source: {input_dir}/{excel_filename}\n\n")
        yaml.dump(rules_data, f, default_flow_style=False, allow_unicode=True)
    
    logger.info(f"Generated: {output_path}")
    return output_path


def is_constant_field(field_name, field_description):
    """
    Determine if a derived target field should be marked as constant.
    
    This function uses heuristics to identify operational flags and control fields
    that should have constant values rather than be derived from source data.
    """
    # Convert to lowercase for pattern matching
    field_name_lower = field_name.lower()
    field_desc_lower = field_description.lower()
    
    # Patterns that indicate operational/control fields that should be constants
    constant_patterns = [
        # Common flag patterns
        'flag', 'flg', 'ind', 'indicator', 'control', 'ctrl',
        # Status and state patterns
        'status', 'state', 'active', 'inactive', 'enabled', 'disabled',
        # Lock and block patterns
        'lock', 'block', 'freeze', 'hold',
        # System and processing patterns
        'system', 'process', 'auto', 'manual',
        # Update and modification patterns
        'update', 'modify', 'change', 'create',
        # Version and audit patterns
        'version', 'audit', 'log', 'track',
        # Delete and archival patterns
        'delete', 'archive', 'purge'
    ]
    
    # Check if any pattern matches in field name or description
    field_text = f"{field_name_lower} {field_desc_lower}"
    
    # Strong indicators for constant fields
    if any(pattern in field_text for pattern in constant_patterns):
        return True
    
    # Additional patterns in German/Dutch (common in SAP contexts)
    international_patterns = [
        'kennzeichen', 'merkmal', 'schalter', 'sperre', 'blockierung'
    ]
    
    if any(pattern in field_text for pattern in international_patterns):
        return True
    
    # Check for single character fields (often flags)
    if len(field_name) == 1 and field_name.upper() in 'XYZQWERTY':
        return True
    
    return False


def generate_column_map_yaml(object_name, variant, source_fields, target_fields, excel_source_path, external_audit_matches=None, central_skip_matches=None, central_manual_matches=None):
    """Generate the complete column_map.yaml content with advanced matching and central memory support."""
    
    # Generate timestamp
    timestamp = datetime.now().strftime("%Y%m%d %H%M")
    
    # Import here to avoid circular imports
    from .main import create_advanced_column_mapping
    
    # Create advanced mapping with detailed results (for YAML generation only, without central memory)
    fuzzy_config = FuzzyConfig(
        threshold=0.6,
        max_suggestions=3,
        levenshtein_weight=0.5,
        jaro_winkler_weight=0.5
    )
    
    # For YAML generation, we use the original function without central memory to maintain compatibility
    result = create_advanced_column_mapping(source_fields, target_fields, fuzzy_config)
    if len(result) == 7:
        mapping_lines, exact_matches, fuzzy_matches, unmapped_sources, audit_matches, _, _ = result
    else:
        mapping_lines, exact_matches, fuzzy_matches, unmapped_sources, audit_matches = result
    
    # Use external matches if provided (from run_map_command)
    if external_audit_matches is not None:
        audit_matches = external_audit_matches
    if central_skip_matches is None:
        central_skip_matches = []
    if central_manual_matches is None:
        central_manual_matches = []
    
    # Generate the YAML content
    yaml_content = []
    yaml_content.append(f"# generated by transform-myd-minimal advanced map @ {timestamp}")
    yaml_content.append(f"# source: {excel_source_path}")
    yaml_content.append("# Advanced field matching with exact, synonym, and fuzzy algorithms")
    if central_skip_matches or central_manual_matches:
        yaml_content.append("# Central mapping memory rules applied")
    yaml_content.append("")
    yaml_content.append("## Match fields")
    yaml_content.append("")
    
    # Add all mappings
    for line in mapping_lines:
        yaml_content.append(line)
    
    yaml_content.append("")
    yaml_content.append("")
    yaml_content.append(f"# column_map.yaml – {variant.upper()} datamap (source → target) + descriptions")
    yaml_content.append(f"# object: {variant.upper()}")
    yaml_content.append("# version: 3 (Integrated Workflow)")
    
    # Add detailed mapping comments with advanced match information
    yaml_content.append("# mappings:")
    
    # Process central memory skip rules first
    if central_skip_matches:
        yaml_content.append("#")
        yaml_content.append("# Central Memory Skip Rules Applied:")
        for result in central_skip_matches:
            yaml_content.extend([
                f"# SKIP: {result.source_field}",
                f"#   source_description: \"{result.source_description}\"",
                f"#   skip_reason: \"{result.reason}\"",
                f"#   confidence: {result.confidence_score:.2f}",
                f"#   rule_type: {result.match_type}",
                "#"
            ])
    
    # Process central memory manual mappings
    if central_manual_matches:
        yaml_content.append("#")
        yaml_content.append("# Central Memory Manual Mappings Applied:")
        for result in central_manual_matches:
            yaml_content.extend([
                f"#  - source: {result.source_field}",
                f"#    source_description: \"{result.source_description}\"",
                f"#    target: {result.target_field}",
                f"#    target_description: \"{result.target_description}\"",
                f"#    decision: MANUAL_MAP",
                f"#    confidence: {result.confidence_score:.2f}",
                f"#    match_type: {result.match_type}",
                f"#    rule: copy",
                f"#    reason: \"{result.reason}\"",
                "#"
            ])
    
    # Process automatic exact matches
    for result in exact_matches:
        yaml_content.extend([
            f"#  - source: {result.source_field}",
            f"#    source_description: \"{result.source_description}\"",
            f"#    target: {result.target_field}",
            f"#    target_description: \"{result.target_description}\"",
            f"#    decision: AUTO_MAP",
            f"#    confidence: {result.confidence_score:.2f}",
            f"#    match_type: {result.match_type}",
            f"#    rule: copy",
            f"#    reason: \"{result.reason}\"",
            "#"
        ])
    
    # Process fuzzy/synonym matches
    for result in fuzzy_matches:
        yaml_content.extend([
            f"#  - source: {result.source_field}",
            f"#    source_description: \"{result.source_description}\"",
            f"#    target: {result.target_field}",
            f"#    target_description: \"{result.target_description}\"",
            f"#    decision: AUTO_MAP",
            f"#    confidence: {result.confidence_score:.2f}",
            f"#    match_type: {result.match_type}",
            f"#    algorithm: {result.algorithm or 'N/A'}",
            f"#    rule: copy",
            f"#    reason: \"{result.reason}\"",
            "#"
        ])
    
    # Process audit matches (fuzzy matches to exact-mapped targets)
    if audit_matches:
        yaml_content.append("#")
        yaml_content.append("# Audit matches (fuzzy matches to already exact-mapped targets):")
        for result in audit_matches:
            yaml_content.extend([
                f"# AUDIT: {result.source_field} -> {result.target_field}",
                f"#   source_description: \"{result.source_description}\"",
                f"#   target_description: \"{result.target_description}\"",
                f"#   confidence: {result.confidence_score:.2f}",
                f"#   algorithm: {result.algorithm or 'N/A'}",
                f"#   reason: \"{result.reason}\"",
                "#"
            ])
    
    # Add derived targets section with smart logic
    yaml_content.append("#derived_targets:")
    mapped_targets = {result.target_field for result in exact_matches + fuzzy_matches if result.target_field}
    
    for _, target_row in target_fields.iterrows():
        target_name = target_row['field_name']
        if target_name not in mapped_targets:
            target_desc = target_row['field_description']
            
            # Use smart logic to determine if this should be a constant field
            if is_constant_field(target_name, target_desc):
                # This appears to be an operational flag or control field
                yaml_content.extend([
                    f"#  - target: {target_name}",
                    f"#    target_description: \"{target_desc}\"",
                    f"#    decision: DERIVED",
                    f"#    confidence: 0.90",
                    f"#    match_type: constant_detection",
                    f"#    rule: constant     # kies 'X' om overschrijven te blokkeren, of ' ' (blank) anders",
                    f"#    value: \" \"",
                    f"#    reason: \"Operationele flag; geen semantische bron in source\"",
                    "#"
                ])
            else:
                # This appears to be a regular derived field that needs business logic
                yaml_content.extend([
                    f"#  - target: {target_name}",
                    f"#    target_description: \"{target_desc}\"",
                    f"#    decision: DERIVED",
                    f"#    confidence: 0.80",
                    f"#    match_type: business_logic_required",
                    f"#    rule: derive       # implementeer business logica voor dit veld",
                    f"#    reason: \"Afgeleid veld; vereist business logica implementatie\"",
                    "#"
                ])
    
    # Add unmapped sources section with advanced information
    yaml_content.append("#unmapped_sources:")
    for result in unmapped_sources:
        yaml_content.extend([
            f"#  - source: {result.source_field}",
            f"#    source_description: \"{result.source_description}\"",
            f"#    decision: UNMAPPED",
            f"#    confidence: {result.confidence_score:.2f}",
            f"#    match_type: {result.match_type}",
            f"#    reason: \"{result.reason}\"",
            "#"
        ])
    
    # Add advanced matching statistics
    yaml_content.append("#matching_statistics:")
    yaml_content.append(f"#  total_sources: {len(source_fields)}")
    yaml_content.append(f"#  total_targets: {len(target_fields)}")
    yaml_content.append(f"#  exact_matches: {len(exact_matches)}")
    yaml_content.append(f"#  fuzzy_matches: {len(fuzzy_matches)}")
    yaml_content.append(f"#  unmapped_sources: {len(unmapped_sources)}")
    yaml_content.append(f"#  mapping_coverage: {((len(exact_matches) + len(fuzzy_matches)) / len(source_fields) * 100):.1f}%")
    yaml_content.append("#")
    
    return '\n'.join(yaml_content)


# Migration functions
def generate_migration_structure(base_path, object_code, table_name, df, mapping_results=None):
    """Generate the new multi-file YAML structure in migrations/ directory."""
    if df is None:
        return None
    
    migrations_path = base_path / "migrations"
    object_path = migrations_path / object_code.upper()
    table_path = object_path / table_name.lower()
    
    # Create directory structure
    table_path.mkdir(parents=True, exist_ok=True)
    
    # Generate each file in the new structure
    generated_files = []
    
    # 1. Update objects.yaml catalog
    objects_file = migrations_path / "objects.yaml"
    update_objects_catalog(objects_file, object_code, table_name)
    generated_files.append(objects_file)
    
    # 2. Generate fields.yaml
    fields_file = generate_migration_fields_yaml(table_path, object_code, table_name, df)
    if fields_file:
        generated_files.append(fields_file)
    
    # 3. Generate mappings.yaml (from existing column_map logic)
    mappings_file = generate_migration_mappings_yaml(table_path, object_code, table_name, df, mapping_results)
    if mappings_file:
        generated_files.append(mappings_file)
    
    # 4. Generate validation.yaml
    validation_file = generate_migration_validation_yaml(table_path, object_code, table_name, df)
    if validation_file:
        generated_files.append(validation_file)
    
    # 5. Generate transformations.yaml
    transformations_file = generate_migration_transformations_yaml(table_path, object_code, table_name, df)
    if transformations_file:
        generated_files.append(transformations_file)
    
    return generated_files


def update_objects_catalog(objects_file, object_code, table_name):
    """Update or create the migrations/objects.yaml catalog file."""
    # Load existing data if file exists
    objects_data = {'objects': []}
    if objects_file.exists():
        try:
            with open(objects_file, 'r', encoding='utf-8') as f:
                existing_data = yaml.safe_load(f)
                if existing_data and 'objects' in existing_data:
                    objects_data = existing_data
        except Exception:
            pass  # Use default structure if file is corrupted
    
    # Find or create object entry
    object_entry = None
    for obj in objects_data['objects']:
        if obj.get('code', '').upper() == object_code.upper():
            object_entry = obj
            break
    
    if not object_entry:
        object_entry = {
            'code': object_code.upper(),
            'name': f'{object_code.upper()} Migration Object',
            'tables': []
        }
        objects_data['objects'].append(object_entry)
    
    # Add table if not already present
    table_entry = {
        'name': table_name.lower(),
        'description': f'{object_code.upper()} {table_name.title()} table migration'
    }
    
    # Check if table already exists
    if not any(t['name'] == table_name.lower() for t in object_entry['tables']):
        object_entry['tables'].append(table_entry)
    
    # Write updated catalog
    objects_file.parent.mkdir(parents=True, exist_ok=True)
    with open(objects_file, 'w', encoding='utf-8') as f:
        f.write(f"# SAP Migration Objects Catalog\n")
        f.write(f"# Generated by transform-myd-minimal @ {datetime.now().strftime('%Y%m%d %H%M')}\n")
        f.write("# This file provides an overview of all SAP migration objects and their target tables\n\n")
        yaml.dump(objects_data, f, default_flow_style=False, allow_unicode=True)


def generate_migration_fields_yaml(table_path, object_code, table_name, df):
    """Generate fields.yaml for the new migrations structure."""
    if df is None:
        return None
    
    # Filter for target fields (SAP target structure)
    target_fields = df[df['field'] == 'Target'].copy()
    
    # Generate fields structure optimized for migrations
    fields_data = {
        'object': object_code.upper(),
        'table': table_name.upper(),
        'description': f'{object_code.upper()} {table_name.title()} - {table_name.upper()} Table',
        'fields': []
    }
    
    for _, row in target_fields.iterrows():
        field_info = {
            'name': row.get('field_name', ''),
            'description': row.get('field_description', ''),
            'type': determine_field_type(row),
            'length': row.get('field_length', ''),
            'required': bool(row.get('field_is_mandatory', False)),
            'key': bool(row.get('field_is_key', False))
        }
        
        # Add additional migration-specific metadata
        if pd.notna(row.get('field_default_value')):
            field_info['default'] = row.get('field_default_value')
        
        fields_data['fields'].append(field_info)
    
    # Write to file
    output_path = table_path / "fields.yaml"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(f"# Field Definitions for {object_code.upper()} {table_name.title()} - {table_name.upper()} Table\n")
        f.write(f"# Generated by transform-myd-minimal @ {datetime.now().strftime('%Y%m%d %H%M')}\n")
        f.write(f"# This file defines the structure and metadata of target fields\n\n")
        yaml.dump(fields_data, f, default_flow_style=False, allow_unicode=True)
    
    logger.info(f"Generated: {output_path}")
    return output_path


def generate_migration_mappings_yaml(table_path, object_code, table_name, df, mapping_results=None):
    """Generate mappings.yaml for the new migrations structure."""
    if df is None:
        return None
    
    # Generate mapping data structure
    mappings_data = {
        'object': object_code.upper(),
        'table': table_name.upper(),
        'description': f'Source-to-Target Field Mappings for {object_code.upper()} {table_name.title()}',
        'mappings': []
    }
    
    # If mapping results are provided, use them; otherwise fall back to basic implementation
    if mapping_results:
        exact_matches = mapping_results.get('exact_matches', [])
        fuzzy_matches = mapping_results.get('fuzzy_matches', [])
        central_manual_matches = mapping_results.get('central_manual_matches', [])
        central_skip_matches = mapping_results.get('central_skip_matches', [])
        audit_matches = mapping_results.get('audit_matches', [])
        unmapped_sources = mapping_results.get('unmapped_sources', [])
        source_fields = mapping_results.get('source_fields')
        target_fields = mapping_results.get('target_fields')
        
        # Create mapping entries based on actual matching results
        all_matches = exact_matches + fuzzy_matches + central_manual_matches
        processed_source_fields = set()
        
        # Process exact and fuzzy matches first
        for match in exact_matches + fuzzy_matches:
            # Get target description from target_fields
            target_desc = ''
            if target_fields is not None:
                target_row = target_fields[target_fields['field_name'] == match.target_field]
                if not target_row.empty:
                    target_desc = target_row.iloc[0].get('field_description', '')
            
            mapping_entry = {
                'source_field': match.source_field,
                'source_description': match.source_description,
                'target_field': match.target_field,
                'target_description': target_desc,
                'mapping_type': match.match_type,
                'transformation': 'copy',
                'confidence': match.confidence_score,
                'status': 'mapped'
            }
            mappings_data['mappings'].append(mapping_entry)
            processed_source_fields.add(match.source_field)
        
        # Process central manual matches (skip if already processed by exact/fuzzy)
        for match in central_manual_matches:
            if match.source_field in processed_source_fields:
                continue  # Skip if already processed as exact/fuzzy match
                
            # Get target description from target_fields
            target_desc = match.target_description if hasattr(match, 'target_description') else ''
            if not target_desc and target_fields is not None:
                target_row = target_fields[target_fields['field_name'] == match.target_field]
                if not target_row.empty:
                    target_desc = target_row.iloc[0].get('field_description', '')
            
            mapping_entry = {
                'source_field': match.source_field,
                'source_description': match.source_description,
                'target_field': match.target_field,
                'target_description': target_desc,
                'mapping_type': 'manual',
                'transformation': 'copy',
                'confidence': match.confidence_score,
                'status': 'mapped',
                'reason': match.reason
            }
            mappings_data['mappings'].append(mapping_entry)
            processed_source_fields.add(match.source_field)
        
        # Process skipped fields
        for match in central_skip_matches:
            mapping_entry = {
                'source_field': match.source_field,
                'source_description': match.source_description,
                'target_field': '',
                'target_description': '',
                'mapping_type': 'skip',
                'transformation': 'none',
                'confidence': match.confidence_score,
                'status': 'skipped',
                'reason': match.reason
            }
            mappings_data['mappings'].append(mapping_entry)
            processed_source_fields.add(match.source_field)
        
        # Process unmapped sources (skip if already processed)
        for source_field in unmapped_sources:
            # Handle case where source_field might be an object instead of string
            if hasattr(source_field, 'source_field'):
                source_field_name = source_field.source_field
            else:
                source_field_name = source_field
                
            if source_field_name in processed_source_fields:
                continue  # Skip if already processed
                
            # Get source description from source_fields
            source_desc = ''
            if source_fields is not None:
                source_row = source_fields[source_fields['field_name'] == source_field_name]
                if not source_row.empty:
                    source_desc = source_row.iloc[0].get('field_description', '')
            
            mapping_entry = {
                'source_field': source_field_name,
                'source_description': source_desc,
                'target_field': '',
                'target_description': '',
                'mapping_type': 'unmapped',
                'transformation': 'none',
                'confidence': 0.0,
                'status': 'pending',
                'reason': 'Geen geschikte match gevonden'
            }
            mappings_data['mappings'].append(mapping_entry)
            
    else:
        # Fallback to basic implementation if no mapping results provided
        source_fields = df[df['field'] == 'Source'].copy()
        
        for _, source_row in source_fields.iterrows():
            mapping_entry = {
                'source_field': source_row.get('field_name', ''),
                'source_description': source_row.get('field_description', ''),
                'target_field': '',
                'target_description': '',
                'mapping_type': 'direct',
                'transformation': 'copy',
                'confidence': 0.0,
                'status': 'pending'
            }
            mappings_data['mappings'].append(mapping_entry)
    
    # Write to file
    output_path = table_path / "mappings.yaml"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(f"# Source-to-Target Field Mappings for {object_code.upper()} {table_name.title()} - {table_name.upper()} Table\n")
        f.write(f"# Generated by transform-myd-minimal @ {datetime.now().strftime('%Y%m%d %H%M')}\n")
        f.write(f"# This file defines how source fields map to SAP target fields\n\n")
        
        # Add information about mapping process if results are available
        if mapping_results:
            exact_matches = mapping_results.get('exact_matches', [])
            central_manual_matches = mapping_results.get('central_manual_matches', [])
            
            # Check for any overlaps between exact matches and manual mappings (1:1 override cases)
            exact_source_fields = {match.source_field for match in exact_matches}
            manual_source_fields = {match.source_field for match in central_manual_matches}
            
            if exact_source_fields or central_manual_matches:
                f.write("# Mapping Process Summary:\n")
                if central_manual_matches:
                    f.write(f"# - {len(central_manual_matches)} manual mappings applied from central memory\n")
                if exact_matches:
                    f.write(f"# - {len(exact_matches)} exact matches found\n")
                
                # Check for manual mappings that may have been overridden by exact matches
                # This would happen if there were rules for the same source field
                overridden_manual = []
                for manual_match in central_manual_matches:
                    # Check if this manual mapping's target was also matched by an exact match of a different source
                    for exact_match in exact_matches:
                        if (exact_match.target_field == manual_match.target_field and 
                            exact_match.source_field != manual_match.source_field):
                            overridden_manual.append((manual_match, exact_match))
                
                if overridden_manual:
                    f.write("# Note: Some manual mappings may be redundant due to exact 1:1 matches:\n")
                    for manual_match, exact_match in overridden_manual:
                        f.write(f"#   - Manual mapping {manual_match.source_field}→{manual_match.target_field} "
                               f"has same target as exact match {exact_match.source_field}→{exact_match.target_field}\n")
                f.write("#\n\n")
        
        yaml.dump(mappings_data, f, default_flow_style=False, allow_unicode=True)
    
    logger.info(f"Generated: {output_path}")
    return output_path


def generate_migration_validation_yaml(table_path, object_code, table_name, df):
    """Generate validation.yaml for the new migrations structure."""
    if df is None:
        return None
    
    # Generate validation rules structure
    validation_data = {
        'object': object_code.upper(),
        'table': table_name.upper(),
        'description': f'Data Validation Rules for {object_code.upper()} {table_name.title()}',
        'validation_rules': []
    }
    
    # Filter for target fields and generate validation rules
    target_fields = df[df['field'] == 'Target'].copy()
    
    for _, row in target_fields.iterrows():
        field_name = row.get('field_name', '')
        field_desc = row.get('field_description', '')
        is_mandatory = bool(row.get('field_is_mandatory', False))
        is_key = bool(row.get('field_is_key', False))
        
        # Create validation rules based on field properties
        if is_key:
            validation_data['validation_rules'].append({
                'field': field_name,
                'rule_type': 'primary_key',
                'rule': 'not_null_unique',
                'description': f'Primary key validation for {field_name}',
                'severity': 'error'
            })
        
        if is_mandatory:
            validation_data['validation_rules'].append({
                'field': field_name,
                'rule_type': 'required',
                'rule': 'not_null',
                'description': f'Mandatory field validation for {field_name}',
                'severity': 'error'
            })
        
        # Add format validation based on field type
        field_type = determine_field_type(row)
        if field_type == 'date':
            validation_data['validation_rules'].append({
                'field': field_name,
                'rule_type': 'format',
                'rule': 'valid_date',
                'description': f'Date format validation for {field_name}',
                'severity': 'warning'
            })
    
    # Add table-level validation rules
    validation_data['table_rules'] = [
        {
            'rule_type': 'completeness',
            'rule': 'record_count_check',
            'description': 'Ensure all source records are migrated',
            'severity': 'error'
        },
        {
            'rule_type': 'consistency',
            'rule': 'referential_integrity',
            'description': 'Maintain referential integrity across related tables',
            'severity': 'error'
        }
    ]
    
    # Write to file
    output_path = table_path / "validation.yaml"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(f"# Data Validation Rules for {object_code.upper()} {table_name.title()} - {table_name.upper()} Table\n")
        f.write(f"# Generated by transform-myd-minimal @ {datetime.now().strftime('%Y%m%d %H%M')}\n")
        f.write(f"# This file defines validation rules to ensure data quality\n\n")
        yaml.dump(validation_data, f, default_flow_style=False, allow_unicode=True)
    
    logger.info(f"Generated: {output_path}")
    return output_path


def generate_migration_transformations_yaml(table_path, object_code, table_name, df):
    """Generate transformations.yaml for the new migrations structure."""
    if df is None:
        return None
    
    # Generate transformations structure
    transformations_data = {
        'object': object_code.upper(),
        'table': table_name.upper(),
        'description': f'Value Transformations for {object_code.upper()} {table_name.title()}',
        'transformations': [],
        'lookup_tables': [],
        'business_rules': [],
        'audit_requirements': []
    }
    
    # Filter target fields for transformation rules
    target_fields = df[df['field'] == 'Target'].copy()
    
    for _, row in target_fields.iterrows():
        field_name = row.get('field_name', '')
        field_desc = row.get('field_description', '')
        field_type = determine_field_type(row)
        
        # Create transformation rules based on field characteristics
        if is_operational_field(field_name, field_desc):
            transformations_data['transformations'].append({
                'target_field': field_name,
                'transformation_type': 'constant',
                'source_expression': 'NULL',
                'target_value': ' ',
                'description': f'Set {field_name} to blank for operational field',
                'business_rule': 'Operational fields have no source equivalent'
            })
        elif is_derived_field(field_name, field_desc):
            transformations_data['transformations'].append({
                'target_field': field_name,
                'transformation_type': 'calculated',
                'source_expression': 'TBD',
                'calculation_logic': 'Business logic required',
                'description': f'Calculate {field_name} based on business rules',
                'business_rule': 'Requires business logic implementation'
            })
    
    # Add common business rules
    transformations_data['business_rules'].append({
        'rule_id': 'currency_conversion',
        'description': 'Convert source currency to target currency',
        'applies_to': 'amount_fields',
        'implementation': 'Use exchange rate table'
    })
    
    transformations_data['business_rules'].append({
        'rule_id': 'date_standardization',
        'description': 'Standardize date formats to SAP format',
        'applies_to': 'date_fields',
        'implementation': 'Convert to YYYYMMDD format'
    })
    
    # Add audit requirements
    transformations_data['audit_requirements'].append({
        'track_field': 'all',
        'requirement': 'Log original and transformed values',
        'retention_period': '7_years'
    })
    
    # Write to file
    output_path = table_path / "transformations.yaml"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(f"# Value Transformations for {object_code.upper()} {table_name.title()} - {table_name.upper()} Table\n")
        f.write(f"# Generated by transform-myd-minimal @ {datetime.now().strftime('%Y%m%d %H%M')}\n")
        f.write(f"# This file defines how source values should be transformed to target values\n\n")
        yaml.dump(transformations_data, f, default_flow_style=False, allow_unicode=True)
    
    logger.info(f"Generated: {output_path}")
    return output_path